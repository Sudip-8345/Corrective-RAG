{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOipwmWmmzqIC1dbI+SWVlg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sudip-8345/Corrective-RAG/blob/main/Corrective_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-google-genai langchain_community langchain-core langchain-chroma langchain-text-splitters langchain-huggingface langchain-groq"
      ],
      "metadata": {
        "id": "5oEiLZpfVVJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers"
      ],
      "metadata": {
        "id": "xdcddG0sWU42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrRwwersVPnm"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "print(len(doc_splits))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "bNYk79MTWTdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=embeddings,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "ErcBelnQXytf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "GOOGLE_API_KEY=\"your gemini api key\"\n",
        "GROQ_API_KEY=\"your groq api key\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
      ],
      "metadata": {
        "id": "e7hJ5ZEHalV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Retrieval Grader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    score: float = Field(\n",
        "        description=\"Document's relevancy to the question between 0 and 1 \"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
        "    Give a floating score from 0 to 1 by answering only the float score to indicate whether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"agent memory\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[0].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}).score)\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "HGLHozPQaByi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "\n",
        "from langsmith import Client\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Prompt\n",
        "client = Client()\n",
        "prompt = client.pull_prompt(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ],
      "metadata": {
        "id": "ilxuHIaGbZmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Question Re-writer\n",
        "\n",
        "# LLM\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for web search in one line. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "RiUEpWhHezaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-tavily tavily"
      ],
      "metadata": {
        "id": "C0WNCPjOgxAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TAVILY_API_KEY']=\"your tavily api key\""
      ],
      "metadata": {
        "id": "YaEgxofBhMcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Search\n",
        "from langchain_tavily import TavilySearch\n",
        "web_search_tool = TavilySearch(\n",
        "    max_results=5,\n",
        "    topic=\"general\"\n",
        ")"
      ],
      "metadata": {
        "id": "l_REZyPofwkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Optional, Annotated, List\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "oMThC9jRiTaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphState(TypedDict):\n",
        "  question : str\n",
        "  generation : str\n",
        "  web_search : str\n",
        "  documents : List[str]"
      ],
      "metadata": {
        "id": "HDdEW-WugroN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(state: GraphState):\n",
        "  documents = retriever.invoke(state['question'])\n",
        "  # docs = [doc.metadata['description'] for doc in documents]\n",
        "  return {\"documents\" : documents}"
      ],
      "metadata": {
        "id": "EGu2o7isitxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(state: GraphState):\n",
        "\n",
        "    docs = state[\"documents\"]\n",
        "\n",
        "    if len(docs) == 0:\n",
        "        print(\"⚠️ No documents to generate from\")\n",
        "        return {\"generation\": \"No relevant documents found.\"}\n",
        "\n",
        "    formatted_docs = \"\\n\\n\".join(\n",
        "        doc.page_content for doc in docs\n",
        "    )\n",
        "\n",
        "    ans = rag_chain.invoke({\n",
        "        \"context\": formatted_docs,\n",
        "        \"question\": state[\"question\"]\n",
        "    })\n",
        "\n",
        "    return {\"generation\": ans}"
      ],
      "metadata": {
        "id": "O6FDkryFmXzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_documents(state: GraphState):\n",
        "    web_search = \"no\"\n",
        "    filtered_docs = []\n",
        "    relv = []\n",
        "    for doc in state['documents']:\n",
        "        grade = retrieval_grader.invoke({\n",
        "            \"question\": state[\"question\"],\n",
        "            \"document\": doc.page_content   # ✅ FIXED\n",
        "        })\n",
        "        if grade.score > 0.6:\n",
        "          filtered_docs.append(doc)\n",
        "        relv.append(grade.score)\n",
        "    if any(i>0.70 for i in relv):\n",
        "      web_search = \"yes\"\n",
        "\n",
        "    return {\n",
        "        \"documents\": filtered_docs,\n",
        "        \"web_search\": web_search\n",
        "    }"
      ],
      "metadata": {
        "id": "qSqtL16EpBxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_question(state:GraphState):\n",
        "  refined_question = question_rewriter.invoke(state['question'])\n",
        "  return {'question' : refined_question}"
      ],
      "metadata": {
        "id": "mXl9AGnsxfJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def web_search(state:GraphState):\n",
        "  documents = state['documents']\n",
        "  docs = web_search_tool.invoke(state['question'])\n",
        "  results = docs[\"results\"]\n",
        "  web_res = \"\\n\".join([d[\"content\"] for d in results])\n",
        "  web_res = Document(web_res)\n",
        "  documents.append(web_res)\n",
        "  return {'documents': documents}"
      ],
      "metadata": {
        "id": "NfPkaRq22kXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decide_to_generate(state:GraphState):\n",
        "  if state['web_search'] == 'yes':\n",
        "    print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
        "    return 'transform_query'\n",
        "  else:\n",
        "    print(\"---DECISION: GENERATE---\")\n",
        "    return 'generate'"
      ],
      "metadata": {
        "id": "R38BQr6O4GeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "graph = StateGraph(GraphState)\n",
        "graph.add_node('retrieve', retrieve)\n",
        "graph.add_node('generate',generate)\n",
        "graph.add_node('grade',grade_documents)\n",
        "graph.add_node('web_search', web_search)\n",
        "graph.add_node('transform_query', transform_question)\n",
        "\n",
        "graph.add_edge(START, 'retrieve')\n",
        "graph.add_edge('retrieve', 'grade')\n",
        "graph.add_conditional_edges(\n",
        "    'grade',\n",
        "    decide_to_generate,\n",
        "     {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        'generate' : 'generate'\n",
        "    }\n",
        ")\n",
        "graph.add_edge('transform_query', 'web_search')\n",
        "graph.add_edge('web_search', 'generate')\n",
        "graph.add_edge('generate', END)\n",
        "\n",
        "workflow = graph.compile()\n",
        "workflow"
      ],
      "metadata": {
        "id": "SQvmu-op5GYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\"question\": \"agent memory?\"}\n",
        "final_state = workflow.invoke(inputs)\n",
        "print('Final State : ')\n",
        "print(final_state)"
      ],
      "metadata": {
        "id": "cYYGaqn97row"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d67153a"
      },
      "source": [
        "# Run\n",
        "inputs = {\"question\": \"agent memory?\"}\n",
        "for ans in workflow.stream(inputs):\n",
        "  for key, value in ans.items():\n",
        "    print(f\"Node '{key}':\")\n",
        "  print(\"\\n---\\n\")\n",
        "print('Final Answer : ')\n",
        "print(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\"question\": \"what is ensemble learning\"}\n",
        "for ans in workflow.stream(inputs):\n",
        "  for key, value in ans.items():\n",
        "    print(f\"Node '{key}':\")\n",
        "  print(\"\\n---\\n\")\n",
        "print('Final Answer : ')\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "lwYzD0S0DcRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}